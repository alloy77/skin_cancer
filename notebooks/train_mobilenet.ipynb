{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "First, we need to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Directories\n",
    "\n",
    "Next, we define the directories for the processed data and the saved models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = r\"G:\\OneDrive\\ML-MinorProject\\ISIC-2020 Dataset\"\n",
    "PROCESSED_TRAIN_DIR = os.path.join(BASE_DIR, \"processed_train\")\n",
    "PROCESSED_VAL_DIR = os.path.join(BASE_DIR, \"processed_val\")\n",
    "PROCESSED_TEST_DIR = os.path.join(BASE_DIR, \"processed_test\")\n",
    "\n",
    "MODEL_DIR = r\"G:\\OneDrive\\ML-MinorProject\\models\"\n",
    "MOBILENET_MODEL_PATH = os.path.join(MODEL_DIR, \"mobilenet\", \"mobilenet_best_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Processing Configurations and Data Augmentation\n",
    "\n",
    "We define the image processing configurations and create data generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26500 images belonging to 2 classes.\n",
      "Found 6626 images belonging to 2 classes.\n",
      "Found 10982 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Adjusted hyperparameters for MobileNetV2\n",
    "IMG_SIZE = (224, 224)  # MobileNetV2 input size\n",
    "BATCH_SIZE = 16  # Increased batch size due to smaller input size\n",
    "\n",
    "# Data augmentation and preprocessing configurations\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,  # Preprocessing specific to MobileNetV2, normalized pixel values ranges bw [-1,1]  \n",
    "    rotation_range=20,                        # Reduced rotation range as mobilenetV2 is lightweight in nature\n",
    "    width_shift_range=0.1,                    # Reduced shift ranges (similar reasons)\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,                          # Reduced shear\n",
    "    zoom_range=0.1,                           # Reduced zoom\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Training data generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    PROCESSED_TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Validation data generator\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    PROCESSED_VAL_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Testing data generator\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    PROCESSED_TEST_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Data Augmentation\n",
    "\n",
    "We visualize some augmented images to ensure that the augmentations are applied correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_augmentation(generator):\n",
    "    batch = next(generator)\n",
    "    images = batch[0]  # Get the batch of images (input data)\n",
    "\n",
    "    # Denormalize images to [0, 1] range for visualization\n",
    "    images = (images + 1) / 2.0  # Rescale from [-1, 1] to [0, 1]\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(7):\n",
    "        plt.subplot(1, 7, i + 1)\n",
    "        plt.imshow(images[i])  # Display the image\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(\"Augmented Images\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to visualize augmented images\n",
    "visualize_augmentation(train_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Enable Mixed Precision\n",
    "\n",
    "We enable mixed precision to improve training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define and Compile Model\n",
    "\n",
    "We define and compile the MobileNetV2 model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 1280)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 1281      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,259,265\n",
      "Trainable params: 2,225,153\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the base model for MobileNetV2\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Build the full model\n",
    "model = Sequential([\n",
    "    base_model,                             # MobileNetV2 as the base model\n",
    "    GlobalAveragePooling2D(),               # Pooling layer\n",
    "    Dropout(0.5),                           # Regularization with dropout\n",
    "    Dense(1, activation='sigmoid', dtype='float32')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),   # Adam optimizer with a lower learning rate\n",
    "    loss='binary_crossentropy',            # Loss function for binary classification\n",
    "    metrics=['accuracy']                   # Metric to track during training\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define Callbacks\n",
    "\n",
    "We define callbacks to improve the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Save the best model during training\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='mobilenetv2_best_model.h5',  # Save the model to this file\n",
    "    monitor='val_loss',                   # Monitor validation loss\n",
    "    save_best_only=True,                  # Save only the best model\n",
    "    save_weights_only=False,              # Save the entire model\n",
    "    mode='min',                           # Look for the minimum val_loss\n",
    "    verbose=1                             # Print saving progress\n",
    ")\n",
    "\n",
    "# 2. Stop training early if validation loss stops improving\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',                   # Monitor validation loss\n",
    "    patience=10,                          # Stop after 10 epochs of no improvement\n",
    "    mode='min',                           # Look for the minimum val_loss\n",
    "    restore_best_weights=True,            # Restore the best weights at the end\n",
    "    verbose=1                             # Print early stopping message\n",
    ")\n",
    "\n",
    "# 3. Reduce learning rate when validation loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',                   # Monitor validation loss\n",
    "    factor=0.2,                           # Reduce the learning rate by 20%\n",
    "    patience=5,                           # Wait for 5 epochs before reducing\n",
    "    min_lr=1e-6,                          # Minimum learning rate\n",
    "    mode='min',                           # Look for the minimum val_loss\n",
    "    verbose=1                             # Print learning rate reduction message\n",
    ")\n",
    "\n",
    "# Combine the callbacks\n",
    "callbacks = [checkpoint, early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train the Model\n",
    "\n",
    "We train the model using the data generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0842 - accuracy: 0.9806\n",
      "Epoch 1: val_loss improved from inf to 0.11062, saving model to mobilenetv2_best_model.h5\n",
      "1657/1657 [==============================] - 2729s 2s/step - loss: 0.0842 - accuracy: 0.9806 - val_loss: 0.1106 - val_accuracy: 0.9823 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9824\n",
      "Epoch 2: val_loss did not improve from 0.11062\n",
      "1657/1657 [==============================] - 2719s 2s/step - loss: 0.0738 - accuracy: 0.9824 - val_loss: 0.1281 - val_accuracy: 0.9823 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9824\n",
      "Epoch 3: val_loss improved from 0.11062 to 0.08151, saving model to mobilenetv2_best_model.h5\n",
      "1657/1657 [==============================] - 2693s 2s/step - loss: 0.0696 - accuracy: 0.9824 - val_loss: 0.0815 - val_accuracy: 0.9823 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9823\n",
      "Epoch 4: val_loss did not improve from 0.08151\n",
      "1657/1657 [==============================] - 2671s 2s/step - loss: 0.0658 - accuracy: 0.9823 - val_loss: 0.1200 - val_accuracy: 0.9823 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9825\n",
      "Epoch 5: val_loss did not improve from 0.08151\n",
      "1657/1657 [==============================] - 2695s 2s/step - loss: 0.0648 - accuracy: 0.9825 - val_loss: 0.0836 - val_accuracy: 0.9823 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9824\n",
      "Epoch 6: val_loss did not improve from 0.08151\n",
      "1657/1657 [==============================] - 2713s 2s/step - loss: 0.0632 - accuracy: 0.9824 - val_loss: 0.0867 - val_accuracy: 0.9823 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0602 - accuracy: 0.9824\n",
      "Epoch 7: val_loss did not improve from 0.08151\n",
      "1657/1657 [==============================] - 2680s 2s/step - loss: 0.0602 - accuracy: 0.9824 - val_loss: 0.1097 - val_accuracy: 0.9823 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9824\n",
      "Epoch 8: val_loss did not improve from 0.08151\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "1657/1657 [==============================] - 2713s 2s/step - loss: 0.0576 - accuracy: 0.9824 - val_loss: 0.0819 - val_accuracy: 0.9823 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9824\n",
      "Epoch 9: val_loss did not improve from 0.08151\n",
      "1657/1657 [==============================] - 2706s 2s/step - loss: 0.0490 - accuracy: 0.9824 - val_loss: 0.0861 - val_accuracy: 0.9823 - lr: 2.0000e-05\n",
      "Epoch 10/20\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9824\n",
      "Epoch 10: val_loss did not improve from 0.08151\n",
      "1657/1657 [==============================] - 2684s 2s/step - loss: 0.0433 - accuracy: 0.9824 - val_loss: 0.0833 - val_accuracy: 0.9823 - lr: 2.0000e-05\n",
      "Epoch 11/20\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9826\n",
      "Epoch 11: val_loss did not improve from 0.08151\n",
      "1657/1657 [==============================] - 2667s 2s/step - loss: 0.0418 - accuracy: 0.9826 - val_loss: 0.0866 - val_accuracy: 0.9823 - lr: 2.0000e-05\n",
      "Epoch 12/20\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9848\n",
      "Epoch 12: val_loss did not improve from 0.08151\n",
      "1657/1657 [==============================] - 2642s 2s/step - loss: 0.0376 - accuracy: 0.9848 - val_loss: 0.1017 - val_accuracy: 0.9822 - lr: 2.0000e-05\n",
      "Epoch 13/20\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9862\n",
      "Epoch 13: val_loss did not improve from 0.08151\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "1657/1657 [==============================] - 2693s 2s/step - loss: 0.0346 - accuracy: 0.9862 - val_loss: 0.1006 - val_accuracy: 0.9807 - lr: 2.0000e-05\n",
      "Epoch 13: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Fine-Tune the Model\n",
    "\n",
    "We unfreeze the base model and fine-tune the entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9824\n",
      "Epoch 1: val_loss improved from 0.08151 to 0.07638, saving model to mobilenetv2_best_model.h5\n",
      "1657/1657 [==============================] - 2499s 2s/step - loss: 0.0606 - accuracy: 0.9824 - val_loss: 0.0764 - val_accuracy: 0.9822 - lr: 1.0000e-05\n",
      "Epoch 2/10\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9825\n",
      "Epoch 2: val_loss did not improve from 0.07638\n",
      "1657/1657 [==============================] - 2311s 1s/step - loss: 0.0572 - accuracy: 0.9825 - val_loss: 0.0786 - val_accuracy: 0.9823 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.9824\n",
      "Epoch 3: val_loss improved from 0.07638 to 0.07442, saving model to mobilenetv2_best_model.h5\n",
      "1657/1657 [==============================] - 2327s 1s/step - loss: 0.0553 - accuracy: 0.9824 - val_loss: 0.0744 - val_accuracy: 0.9825 - lr: 1.0000e-05\n",
      "Epoch 4/10\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9828\n",
      "Epoch 4: val_loss improved from 0.07442 to 0.07091, saving model to mobilenetv2_best_model.h5\n",
      "1657/1657 [==============================] - 2325s 1s/step - loss: 0.0529 - accuracy: 0.9828 - val_loss: 0.0709 - val_accuracy: 0.9819 - lr: 1.0000e-05\n",
      "Epoch 5/10\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9831\n",
      "Epoch 5: val_loss did not improve from 0.07091\n",
      "1657/1657 [==============================] - 2357s 1s/step - loss: 0.0494 - accuracy: 0.9831 - val_loss: 0.0806 - val_accuracy: 0.9823 - lr: 1.0000e-05\n",
      "Epoch 6/10\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9836\n",
      "Epoch 6: val_loss did not improve from 0.07091\n",
      "1657/1657 [==============================] - 2346s 1s/step - loss: 0.0478 - accuracy: 0.9836 - val_loss: 0.0840 - val_accuracy: 0.9823 - lr: 1.0000e-05\n",
      "Epoch 7/10\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9839\n",
      "Epoch 7: val_loss did not improve from 0.07091\n",
      "1657/1657 [==============================] - 2360s 1s/step - loss: 0.0476 - accuracy: 0.9839 - val_loss: 0.0845 - val_accuracy: 0.9822 - lr: 1.0000e-05\n",
      "Epoch 8/10\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9844\n",
      "Epoch 8: val_loss did not improve from 0.07091\n",
      "1657/1657 [==============================] - 2347s 1s/step - loss: 0.0450 - accuracy: 0.9844 - val_loss: 0.0829 - val_accuracy: 0.9817 - lr: 1.0000e-05\n",
      "Epoch 9/10\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9844\n",
      "Epoch 9: val_loss did not improve from 0.07091\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
      "1657/1657 [==============================] - 2356s 1s/step - loss: 0.0431 - accuracy: 0.9844 - val_loss: 0.0857 - val_accuracy: 0.9814 - lr: 1.0000e-05\n",
      "Epoch 10/10\n",
      "1657/1657 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9852\n",
      "Epoch 10: val_loss did not improve from 0.07091\n",
      "1657/1657 [==============================] - 2382s 1s/step - loss: 0.0391 - accuracy: 0.9852 - val_loss: 0.0864 - val_accuracy: 0.9814 - lr: 2.0000e-06\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save the Final Model\n",
    "\n",
    "We save the final model after training and fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved to G:\\OneDrive\\ML-MinorProject\\models\\mobilenet\\mobilenet_final_model.h5\n"
     ]
    }
   ],
   "source": [
    "final_model_path = os.path.join(MODEL_DIR, \"mobilenet\", \"mobilenet_final_model.h5\")\n",
    "model.save(final_model_path)\n",
    "print(f\"Final model saved to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
